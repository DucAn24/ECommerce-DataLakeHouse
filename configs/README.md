# ğŸ› ï¸ Configurations Directory

ThÆ° má»¥c nÃ y chá»©a táº¥t cáº£ cÃ¡c file cáº¥u hÃ¬nh cho cÃ¡c services trong Data Lakehouse. Táº¥t cáº£ cÃ¡c cáº¥u hÃ¬nh inline trong docker-compose.yml Ä‘Ã£ Ä‘Æ°á»£c tÃ¡ch ra thÃ nh cÃ¡c file riÃªng Ä‘á»ƒ dá»… quáº£n lÃ½ vÃ  customize.

## ğŸ“ Cáº¥u trÃºc

```
configs/
â”œâ”€â”€ airflow/                     # Apache Airflow configurations
â”‚   â””â”€â”€ airflow.env             # Environment variables cho Airflow
â”œâ”€â”€ flink/                       # Apache Flink configurations  
â”‚   â””â”€â”€ flink.properties        # Flink cluster configuration
â”œâ”€â”€ spark/                       # Apache Spark configurations
â”‚   â””â”€â”€ spark-defaults.conf     # Spark default configurations with Iceberg
â”œâ”€â”€ trino/                       # Trino query engine configurations
â”‚   â”œâ”€â”€ config.properties       # Coordinator configuration
â”‚   â”œâ”€â”€ node.properties         # Node configuration
â”‚   â”œâ”€â”€ worker-config.properties # Worker configuration
â”‚   â”œâ”€â”€ worker-node.properties  # Worker node properties
â”‚   â””â”€â”€ catalog/                # Data catalog configurations
â”‚       â”œâ”€â”€ hive.properties     # Hive Metastore catalog
â”‚       â”œâ”€â”€ delta.properties    # Delta Lake catalog
â”‚       â””â”€â”€ iceberg.properties  # Iceberg catalog
â”œâ”€â”€ kafka-connect/               # Kafka Connect configurations
â”‚   â”œâ”€â”€ worker.properties       # Connect worker configuration
â”‚   â”œâ”€â”€ debezium-postgres.json  # Debezium CDC connector
â”‚   â””â”€â”€ init-connectors.sh      # Connector installation script
â”œâ”€â”€ minio/                       # MinIO object storage configurations
â”‚   â””â”€â”€ setup-buckets.sh       # Bucket creation script
â”œâ”€â”€ superset/                    # Apache Superset BI configurations
â”‚   â”œâ”€â”€ init-superset.sh       # Superset initialization
â”‚   â”œâ”€â”€ worker.sh              # Celery worker script
â”‚   â””â”€â”€ beat.sh                # Celery beat scheduler
â”œâ”€â”€ filebeat/                    # Filebeat log collection
â”‚   â””â”€â”€ filebeat.yml           # Filebeat configuration
â”œâ”€â”€ logstash/                    # Logstash log processing
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â””â”€â”€ logstash.conf      # Processing pipeline
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ logstash.yml       # Logstash configuration
â”œâ”€â”€ prometheus/                  # Prometheus monitoring
â”‚   â””â”€â”€ prometheus.yml         # Monitoring targets
â””â”€â”€ grafana/                     # Grafana dashboards (future)
```

## ğŸ”§ CÃ¡c loáº¡i Configuration

### 1. **Environment Files (.env)**
- **airflow/airflow.env**: Táº¥t cáº£ environment variables cho Airflow services
- ÄÆ°á»£c load báº±ng `env_file` trong docker-compose.yml

### 2. **Properties Files (.properties, .conf)**
- **flink/flink.properties**: Flink cluster settings, checkpointing, memory
- **spark/spark-defaults.conf**: Spark vá»›i Iceberg vÃ  S3 configuration  
- **trino/**: Trino coordinator, worker, vÃ  catalog configurations
- **kafka-connect/worker.properties**: Kafka Connect worker settings

### 3. **Shell Scripts (.sh)**
- **minio/setup-buckets.sh**: Táº¡o vÃ  cáº¥u hÃ¬nh buckets
- **superset/init-superset.sh**: Khá»Ÿi táº¡o Superset vá»›i admin user
- **superset/worker.sh**: Celery worker cho async queries
- **superset/beat.sh**: Celery beat cho scheduled tasks
- **kafka-connect/init-connectors.sh**: CÃ i Ä‘áº·t vÃ  khá»Ÿi Ä‘á»™ng connectors

### 4. **YAML Configurations (.yml)**
- **filebeat/filebeat.yml**: Log collection vÃ  Kafka output
- **logstash/config/logstash.yml**: Logstash server configuration  
- **prometheus/prometheus.yml**: Monitoring targets vÃ  scraping

### 5. **JSON Configurations (.json)**
- **kafka-connect/debezium-postgres.json**: Debezium CDC connector definition

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### Customize Configurations

```bash
# Edit Flink settings
vi configs/flink/flink.properties

# Modify Spark configurations
vi configs/spark/spark-defaults.conf

# Update Trino catalogs
vi configs/trino/catalog/hive.properties

# Change Airflow settings
vi configs/airflow/airflow.env
```

### Test Configuration Changes

```bash
# Restart specific service after config change
docker-compose restart flink-jobmanager flink-taskmanager

# Reload Trino after catalog changes  
docker-compose restart trino-coordinator trino-worker

# Restart Spark after Iceberg config changes
docker-compose restart spark-master spark-worker
```

### Add New Configurations

1. **Create config file**: Add new `.properties`, `.conf`, hoáº·c `.sh` file
2. **Update docker-compose.yml**: Mount config file as volume
3. **Update service**: Use config file trong service startup

Example:
```yaml
services:
  my-service:
    volumes:
      - ./configs/my-service/config.conf:/etc/my-service/config.conf
```

## ğŸ“‹ Configuration Details

### Airflow Environment
- **Database**: PostgreSQL connection vá»›i persistent storage
- **Executor**: CeleryExecutor with Redis broker  
- **Additional Providers**: Spark, Elasticsearch, Amazon S3
- **Security**: Basic authentication enabled

### Flink Properties
- **Checkpointing**: 60s intervals vá»›i filesystem backend
- **Memory**: JobManager 2GB, TaskManager 1.5GB
- **Parallelism**: Default 2, 4 slots per TaskManager
- **Kafka**: Bootstrap servers configured

### Spark Defaults
- **Iceberg**: SparkSessionExtensions enabled
- **Catalogs**: Hive Metastore vÃ  local Hadoop catalog
- **S3**: MinIO endpoint vá»›i credentials
- **Performance**: Adaptive query execution enabled

### Trino Catalogs
- **Hive**: Thrift metastore vá»›i S3 storage
- **Delta Lake**: Delta tables support
- **Iceberg**: Apache Iceberg tables
- **Security**: Read-only mode enabled [[memory:4765761]]

### Kafka Connect
- **Connectors**: Debezium PostgreSQL, S3 Sink, Elasticsearch
- **Converters**: JSON without schemas
- **Topics**: Auto-created vá»›i replication factor 1

## ğŸ” Troubleshooting

### Configuration Not Loading
```bash
# Check file permissions
ls -la configs/service-name/

# Verify mount points
docker-compose config

# Check service logs
docker-compose logs service-name
```

### Invalid Configuration
```bash
# Validate YAML files
yamllint configs/prometheus/prometheus.yml

# Check properties syntax
grep -v "^#" configs/flink/flink.properties

# Test shell scripts
bash -n configs/minio/setup-buckets.sh
```

### Service Restart Issues
```bash
# Remove container vÃ  recreate
docker-compose rm service-name
docker-compose up -d service-name

# Check dependency issues
docker-compose ps
```

## ğŸ’¡ Best Practices

1. **Version Control**: LuÃ´n commit config changes
2. **Backup**: Backup configs trÆ°á»›c khi thay Ä‘á»•i major
3. **Documentation**: Document custom configurations
4. **Testing**: Test trong dev environment trÆ°á»›c production
5. **Security**: KhÃ´ng commit passwords/secrets vÃ o git

## ğŸ”— Related Files

- `docker-compose.yml`: Main orchestration file
- `scripts/setup/`: Deployment scripts
- `sql/init/`: Database initialization  
- `airflow/dags/`: Workflow definitions

---

**Note**: Táº¥t cáº£ configurations Ä‘Æ°á»£c thiáº¿t káº¿ cho development environment. Äá»ƒ production, cáº§n review security settings vÃ  resource allocations.
