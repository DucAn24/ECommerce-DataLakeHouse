input {
  kafka {
    bootstrap_servers => "kafka:29092"
    topics => ["filebeat-logs", "ecommerce-cdc"]
    consumer_threads => 3
    decorate_events => true
    codec => json
  }
}

filter {
  if [@metadata][kafka][topic] == "filebeat-logs" {
    # Parse common log formats
    if [fields][logtype] == "application" {
      grok {
        match => { "message" => "%{COMBINEDAPACHELOG}" }
      }
      date {
        match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
      }
    }
    
    # Add processing timestamp
    mutate {
      add_field => { "processed_at" => "%{@timestamp}" }
      add_field => { "pipeline" => "logstash" }
    }
  }
  
  if [@metadata][kafka][topic] == "ecommerce-cdc" {
    # Process CDC events from Debezium
    if [payload][op] {
      mutate {
        add_field => { "cdc_operation" => "%{[payload][op]}" }
        add_field => { "table_name" => "%{[payload][source][table]}" }
        add_field => { "database_name" => "%{[payload][source][db]}" }
      }
    }
  }
}

output {
  # Send processed data to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logstash-processed-%{+YYYY.MM.dd}"
  }
  
  # Send to different Kafka topics based on processing
  kafka {
    bootstrap_servers => "kafka:29092"
    topic_id => "processed-logs"
    codec => json
  }
  
  # Debug output
  stdout { 
    codec => rubydebug 
  }
}
